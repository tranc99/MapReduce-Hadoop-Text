"word	1
(IntWritable	1
(itr.hasMoreTokens())	1
+=	1
0	1
0;	1
1);	1
:	2
=	7
?	1
Configuration	1
Configuration();	1
Context	2
Exception	1
FileInputFormat.addInputPath(job,	1
FileOutputFormat.setOutputPath(job,	1
IOException,	2
IntSumReducer	1
IntWritable	2
IntWritable();	1
IntWritable(1);	1
IntWritable,	1
IntWritable>	2
InterruptedException	2
Iterable<IntWritable>	1
Job	1
Job.getInstance(conf,	1
Mapper<Object,	1
Path(args[0]));	1
Path(args[1]));	1
Reducer<Text,	1
StringTokenizer	1
StringTokenizer(value.toString());	1
System.exit(job.waitForCompletion(true)	1
Text	2
Text();	1
Text,	3
TokenizerMapper	1
WordCounter	1
args)	1
class	3
conf	1
context)	2
context.write(key,	1
context.write(word,	1
count");	1
extends	2
final	1
for	1
import	11
int	1
itr	1
java.io.IOException;	1
java.util.StringTokenizer;	1
job	1
job.setCombinerClass(IntSumReducer.class);	1
job.setJarByClass(WordCounter.class);	1
job.setMapperClass(TokenizerMapper.class);	1
job.setOutputKeyClass(Text.class);	1
job.setOutputValueClass(IntWritable.class);	1
job.setReducerClass(IntSumReducer.class);	1
key,	2
main(String[]	1
map(Object	1
new	7
one	1
one);	1
org.apache.hadoop.conf.Configuration;	1
org.apache.hadoop.fs.Path;	1
org.apache.hadoop.io.IntWritable;	1
org.apache.hadoop.io.Text;	1
org.apache.hadoop.mapreduce.Job;	1
org.apache.hadoop.mapreduce.Mapper;	1
org.apache.hadoop.mapreduce.Reducer;	1
org.apache.hadoop.mapreduce.lib.input.FileInputFormat;	1
org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;	1
private	3
public	6
reduce(Text	1
result	1
result);	1
result.set(sum);	1
static	4
sum	2
throws	3
val	1
val.get();	1
value,	1
values)	1
values,	1
void	3
while	1
word	1
word.set(itr.nextToken());	1
{	8
}	8
